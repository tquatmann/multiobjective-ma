Storm 1.0.0

Command line arguments: --prism models/mdp/consensus/consensus3_5_2.nm --prop models/mdp/consensus/consensus3_5_2_pareto.pctl --multiobjective:precision 0.001 -tm -stats 
Current working directory: /home/storm/Desktop

Time for model construction: 3.542s.

-------------------------------------------------------------- 
Model type: 	MDP (sparse)
States: 	181129
Transitions: 	487456
Choices: 	487456
Reward Models:  none
Labels: 	4
   * init -> 1 state(s)
   * deadlock -> 0 state(s)
   * one_proc_err -> 25152 state(s)
   * one_coin_ok -> 82233 state(s)
choice labels: 	no
-------------------------------------------------------------- 

Model checking property multi(Pmax=? [F "one_proc_err"], Pmax=? [G "one_coin_ok"]) ...
Preprocessing done in 2.434s seconds.
 Result: 
---------------------------------------------------------------------------------------------------------------------------------------
                                                       Multi-objective Query                                              
---------------------------------------------------------------------------------------------------------------------------------------

Original Formula: 
--------------------------------------------------------------
	multi(Pmax=? [F "one_proc_err"], Pmax=? [G "one_coin_ok"])

Objectives:
--------------------------------------------------------------
00000Pmax=? [F "one_proc_err"] 	(toOrigVal:001*x +000, 	intern threshold:   none, 	intern reward model: objective1 (positive), 	time bounds: none)
000000Pmax=? [G "one_coin_ok"] 	(toOrigVal:001*x +001, 	intern threshold:   none, 	intern reward model: objective2 (negative), 	time bounds: none)
--------------------------------------------------------------

Original Model Information:
-------------------------------------------------------------- 
Model type: 	MDP (sparse)
States: 	181129
Transitions: 	487456
Choices: 	487456
Reward Models:  none
Labels: 	4
   * init -> 1 state(s)
   * deadlock -> 0 state(s)
   * one_proc_err -> 25152 state(s)
   * one_coin_ok -> 82233 state(s)
choice labels: 	no
-------------------------------------------------------------- 

Preprocessed Model Information:
-------------------------------------------------------------- 
Model type: 	MDP (sparse)
States: 	181129
Transitions: 	487456
Choices: 	487456
Reward Models:  objective2, objective1
Labels: 	4
   * init -> 1 state(s)
   * one_proc_err -> 25152 state(s)
   * one_coin_ok -> 82233 state(s)
   * deadlock -> 0 state(s)
choice labels: 	no
-------------------------------------------------------------- 

---------------------------------------------------------------------------------------------------------------------------------------

Solving multi-objective query took 3.491s seconds (consisting of 2.434s seconds for preprocessing and 1.054s seconds for value iteration-based exploration of achievable points).
Result (initial states): 
Underapproximation of achievable values: Polytope with 3 Halfspaces:
   (         1,          0) * x <= 1
   (         0,          1) * x <= 1
   (         1,          1) * x <= 1

Overapproximation of achievable values: Polytope with 3 Halfspaces:
   (         1,          0) * x <= 1
   (         0,          1) * x <= 1
   (       0.5,        0.5) * x <= 0.5

2 pareto optimal points found (Note that these points are safe, i.e., contained in the underapproximation, but there is no guarantee for optimality):
   (0000000000, 0000000001 )
   (0000000001, -2.26485497e-14 )


Time for model checking: 3.494s.

Performance statistics:
  * peak memory usage: 227MB
  * CPU time: 6.880s
  * wallclock time: 7.056s

real	0m7.076s
user	0m6.884s
sys	0m0.144s
