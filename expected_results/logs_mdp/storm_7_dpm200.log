Storm 1.0.0

Command line arguments: --prism models/mdp/dpm/dpm200.nm --prop models/mdp/dpm/dpm200_pareto.pctl --multiobjective:precision 0.001 -tm -stats 
Current working directory: /home/storm/Desktop

Time for model construction: 0.142s.

-------------------------------------------------------------- 
Model type: 	MDP (sparse)
States: 	636
Transitions: 	2550
Choices: 	1860
Reward Models:  queue, power
Labels: 	2
   * deadlock -> 0 state(s)
   * init -> 1 state(s)
choice labels: 	no
-------------------------------------------------------------- 

Model checking property multi(R[exp]{"power"}min=? [C<=200], R[exp]{"queue"}min=? [C<=200]) ...
Preprocessing done in 0.000s seconds.
 Result: 
---------------------------------------------------------------------------------------------------------------------------------------
                                                       Multi-objective Query                                              
---------------------------------------------------------------------------------------------------------------------------------------

Original Formula: 
--------------------------------------------------------------
	multi(R[exp]{"power"}min=? [C<=200], R[exp]{"queue"}min=? [C<=200])

Objectives:
--------------------------------------------------------------
0R[exp]{"power"}min=? [C<=200] 	(toOrigVal:0-1*x +000, 	intern threshold:   none, 	intern reward model: objective1 (negative), 	time bounds:<=00200)
0R[exp]{"queue"}min=? [C<=200] 	(toOrigVal:0-1*x +000, 	intern threshold:   none, 	intern reward model: objective2 (negative), 	time bounds:<=00200)
--------------------------------------------------------------

Original Model Information:
-------------------------------------------------------------- 
Model type: 	MDP (sparse)
States: 	636
Transitions: 	2550
Choices: 	1860
Reward Models:  queue, power
Labels: 	2
   * deadlock -> 0 state(s)
   * init -> 1 state(s)
choice labels: 	no
-------------------------------------------------------------- 

Preprocessed Model Information:
-------------------------------------------------------------- 
Model type: 	MDP (sparse)
States: 	636
Transitions: 	2550
Choices: 	1860
Reward Models:  objective2, objective1
Labels: 	2
   * deadlock -> 0 state(s)
   * init -> 1 state(s)
choice labels: 	no
-------------------------------------------------------------- 

---------------------------------------------------------------------------------------------------------------------------------------

Solving multi-objective query took 0.159s seconds (consisting of 0.000s seconds for preprocessing and 0.158s seconds for value iteration-based exploration of achievable points).
Result (initial states): 
Underapproximation of achievable values: Polytope with 5 Halfspaces:
   (-0.0615529,         -1) * x <= -173.537
   (         0,         -1) * x <= -158.306
   (        -1,          0) * x <= -10
   (-0.0623304,         -1) * x <= -173.566
   (-0.0616902,         -1) * x <= -173.545

Overapproximation of achievable values: Polytope with 8 Halfspaces:
   (        -1,          0) * x <= -10
   (         0,         -1) * x <= -158.306
   (-0.0580294,  -0.941971) * x <= -163.469
   (-0.0584063,  -0.941594) * x <= -163.419
   (-0.0579407,  -0.942059) * x <= -163.471
   (-0.0586733,  -0.941327) * x <= -163.383
   (-0.0579839,  -0.942016) * x <= -163.475
   (-0.0581057,  -0.941894) * x <= -163.461

4 pareto optimal points found (Note that these points are safe, i.e., contained in the underapproximation, but there is no guarantee for optimality):
   (0000000010, 172.9430909 )
   (33.97502894, 171.4487172 )
   (055.284907, 170.1341062 )
   (247.4478595, 158.3059108 )


Time for model checking: 0.160s.

Performance statistics:
  * peak memory usage: 214MB
  * CPU time: 0.224s
  * wallclock time: 0.316s

real	0m0.342s
user	0m0.224s
sys	0m0.084s
