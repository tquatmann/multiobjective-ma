Storm 1.0.0

Command line arguments: --prism models/mdp/dpm/dpm300.nm --prop models/mdp/dpm/dpm300_pareto.pctl --multiobjective:precision 0.001 -tm -stats 
Current working directory: /home/storm/Desktop

Time for model construction: 0.105s.

-------------------------------------------------------------- 
Model type: 	MDP (sparse)
States: 	636
Transitions: 	2550
Choices: 	1860
Reward Models:  queue, power
Labels: 	2
   * deadlock -> 0 state(s)
   * init -> 1 state(s)
choice labels: 	no
-------------------------------------------------------------- 

Model checking property multi(R[exp]{"power"}min=? [C<=300], R[exp]{"queue"}min=? [C<=300]) ...
Preprocessing done in 0.000s seconds.
 Result: 
---------------------------------------------------------------------------------------------------------------------------------------
                                                       Multi-objective Query                                              
---------------------------------------------------------------------------------------------------------------------------------------

Original Formula: 
--------------------------------------------------------------
	multi(R[exp]{"power"}min=? [C<=300], R[exp]{"queue"}min=? [C<=300])

Objectives:
--------------------------------------------------------------
0R[exp]{"power"}min=? [C<=300] 	(toOrigVal:0-1*x +000, 	intern threshold:   none, 	intern reward model: objective1 (negative), 	time bounds:<=00300)
0R[exp]{"queue"}min=? [C<=300] 	(toOrigVal:0-1*x +000, 	intern threshold:   none, 	intern reward model: objective2 (negative), 	time bounds:<=00300)
--------------------------------------------------------------

Original Model Information:
-------------------------------------------------------------- 
Model type: 	MDP (sparse)
States: 	636
Transitions: 	2550
Choices: 	1860
Reward Models:  queue, power
Labels: 	2
   * deadlock -> 0 state(s)
   * init -> 1 state(s)
choice labels: 	no
-------------------------------------------------------------- 

Preprocessed Model Information:
-------------------------------------------------------------- 
Model type: 	MDP (sparse)
States: 	636
Transitions: 	2550
Choices: 	1860
Reward Models:  objective2, objective1
Labels: 	2
   * deadlock -> 0 state(s)
   * init -> 1 state(s)
choice labels: 	no
-------------------------------------------------------------- 

---------------------------------------------------------------------------------------------------------------------------------------

Solving multi-objective query took 0.167s seconds (consisting of 0.000s seconds for preprocessing and 0.167s seconds for value iteration-based exploration of achievable points).
Result (initial states): 
Underapproximation of achievable values: Polytope with 4 Halfspaces:
   (        -1,          0) * x <= -15
   (-0.0931068,         -1) * x <= -274.327
   (         0,         -1) * x <= -239.656
   (-0.0934162,         -1) * x <= -274.342

Overapproximation of achievable values: Polytope with 6 Halfspaces:
   (        -1,          0) * x <= -15
   (         0,         -1) * x <= -239.656
   (-0.0851543,  -0.914846) * x <= -250.958
   (-0.0852025,  -0.914797) * x <= -250.955
   (-0.0854352,  -0.914565) * x <= -250.904
   (-0.0851763,  -0.914824) * x <= -250.96

3 pareto optimal points found (Note that these points are safe, i.e., contained in the underapproximation, but there is no guarantee for optimality):
   (51.20796399, 269.5587779 )
   (372.3787131, 239.6556023 )
   (0000000015, 272.9411884 )


Time for model checking: 0.168s.

Performance statistics:
  * peak memory usage: 214MB
  * CPU time: 0.208s
  * wallclock time: 0.286s

real	0m0.309s
user	0m0.208s
sys	0m0.080s
